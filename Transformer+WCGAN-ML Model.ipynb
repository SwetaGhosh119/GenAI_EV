{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61fa4c47-6a1a-4256-acf0-bb96d6b886d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import io\n",
    "import shutil\n",
    "import os\n",
    "import sys\n",
    "from os import path\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91359b41-7f8d-43d1-be51-3aa94acf903c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>irq_softirq_exit</th>\n",
       "      <th>irq_softirq_entry</th>\n",
       "      <th>irq_softirq_raise</th>\n",
       "      <th>kmem_kmem_cache_free</th>\n",
       "      <th>kmem_kmem_cache_alloc</th>\n",
       "      <th>net_netif_rx</th>\n",
       "      <th>net_netif_rx_ni_exit</th>\n",
       "      <th>net_netif_rx_ni_entry</th>\n",
       "      <th>rpm_rpm_usage</th>\n",
       "      <th>rpm_rpm_resume</th>\n",
       "      <th>...</th>\n",
       "      <th>Scenario_DoS</th>\n",
       "      <th>Scenario_Recon</th>\n",
       "      <th>Label_attack</th>\n",
       "      <th>Label_benign</th>\n",
       "      <th>interface_any</th>\n",
       "      <th>interface_iso15118</th>\n",
       "      <th>interface_iso15118</th>\n",
       "      <th>interface_none</th>\n",
       "      <th>interface_ocpp</th>\n",
       "      <th>isDoS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5808</td>\n",
       "      <td>5808</td>\n",
       "      <td>5826</td>\n",
       "      <td>3976</td>\n",
       "      <td>4016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4277</td>\n",
       "      <td>4369</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4791</td>\n",
       "      <td>4791</td>\n",
       "      <td>4808</td>\n",
       "      <td>12217</td>\n",
       "      <td>13581</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1355</td>\n",
       "      <td>1391</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6635</td>\n",
       "      <td>6635</td>\n",
       "      <td>6667</td>\n",
       "      <td>16222</td>\n",
       "      <td>16487</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2683</td>\n",
       "      <td>2719</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9165</td>\n",
       "      <td>9165</td>\n",
       "      <td>9228</td>\n",
       "      <td>15833</td>\n",
       "      <td>17867</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4934</td>\n",
       "      <td>4988</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8405</td>\n",
       "      <td>8405</td>\n",
       "      <td>8431</td>\n",
       "      <td>16182</td>\n",
       "      <td>15720</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4736</td>\n",
       "      <td>4778</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6161</th>\n",
       "      <td>95032</td>\n",
       "      <td>95032</td>\n",
       "      <td>95043</td>\n",
       "      <td>109798</td>\n",
       "      <td>109887</td>\n",
       "      <td>44337</td>\n",
       "      <td>44335</td>\n",
       "      <td>44335</td>\n",
       "      <td>80240</td>\n",
       "      <td>80266</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6162</th>\n",
       "      <td>96744</td>\n",
       "      <td>96744</td>\n",
       "      <td>96779</td>\n",
       "      <td>98202</td>\n",
       "      <td>98220</td>\n",
       "      <td>37797</td>\n",
       "      <td>37795</td>\n",
       "      <td>37795</td>\n",
       "      <td>69827</td>\n",
       "      <td>69873</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6163</th>\n",
       "      <td>99936</td>\n",
       "      <td>99935</td>\n",
       "      <td>99947</td>\n",
       "      <td>105024</td>\n",
       "      <td>104998</td>\n",
       "      <td>41208</td>\n",
       "      <td>41208</td>\n",
       "      <td>41208</td>\n",
       "      <td>75511</td>\n",
       "      <td>75531</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6164</th>\n",
       "      <td>96575</td>\n",
       "      <td>96576</td>\n",
       "      <td>96590</td>\n",
       "      <td>99527</td>\n",
       "      <td>99583</td>\n",
       "      <td>38825</td>\n",
       "      <td>38825</td>\n",
       "      <td>38825</td>\n",
       "      <td>71057</td>\n",
       "      <td>71074</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6165</th>\n",
       "      <td>97650</td>\n",
       "      <td>97650</td>\n",
       "      <td>97661</td>\n",
       "      <td>102640</td>\n",
       "      <td>102628</td>\n",
       "      <td>40453</td>\n",
       "      <td>40453</td>\n",
       "      <td>40453</td>\n",
       "      <td>72933</td>\n",
       "      <td>72950</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6166 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      irq_softirq_exit  irq_softirq_entry  irq_softirq_raise  \\\n",
       "0                 5808               5808               5826   \n",
       "1                 4791               4791               4808   \n",
       "2                 6635               6635               6667   \n",
       "3                 9165               9165               9228   \n",
       "4                 8405               8405               8431   \n",
       "...                ...                ...                ...   \n",
       "6161             95032              95032              95043   \n",
       "6162             96744              96744              96779   \n",
       "6163             99936              99935              99947   \n",
       "6164             96575              96576              96590   \n",
       "6165             97650              97650              97661   \n",
       "\n",
       "      kmem_kmem_cache_free  kmem_kmem_cache_alloc  net_netif_rx  \\\n",
       "0                     3976                   4016             0   \n",
       "1                    12217                  13581             0   \n",
       "2                    16222                  16487             0   \n",
       "3                    15833                  17867             0   \n",
       "4                    16182                  15720             0   \n",
       "...                    ...                    ...           ...   \n",
       "6161                109798                 109887         44337   \n",
       "6162                 98202                  98220         37797   \n",
       "6163                105024                 104998         41208   \n",
       "6164                 99527                  99583         38825   \n",
       "6165                102640                 102628         40453   \n",
       "\n",
       "      net_netif_rx_ni_exit  net_netif_rx_ni_entry  rpm_rpm_usage  \\\n",
       "0                        0                      0           4277   \n",
       "1                        0                      0           1355   \n",
       "2                        0                      0           2683   \n",
       "3                        0                      0           4934   \n",
       "4                        0                      0           4736   \n",
       "...                    ...                    ...            ...   \n",
       "6161                 44335                  44335          80240   \n",
       "6162                 37795                  37795          69827   \n",
       "6163                 41208                  41208          75511   \n",
       "6164                 38825                  38825          71057   \n",
       "6165                 40453                  40453          72933   \n",
       "\n",
       "      rpm_rpm_resume  ...  Scenario_DoS  Scenario_Recon  Label_attack  \\\n",
       "0               4369  ...         False           False          True   \n",
       "1               1391  ...         False           False          True   \n",
       "2               2719  ...         False           False          True   \n",
       "3               4988  ...         False           False          True   \n",
       "4               4778  ...         False           False          True   \n",
       "...              ...  ...           ...             ...           ...   \n",
       "6161           80266  ...          True           False          True   \n",
       "6162           69873  ...          True           False          True   \n",
       "6163           75531  ...          True           False          True   \n",
       "6164           71074  ...          True           False          True   \n",
       "6165           72950  ...          True           False          True   \n",
       "\n",
       "      Label_benign  interface_any  interface_iso15118  interface_iso15118   \\\n",
       "0            False           True               False                False   \n",
       "1            False           True               False                False   \n",
       "2            False           True               False                False   \n",
       "3            False           True               False                False   \n",
       "4            False           True               False                False   \n",
       "...            ...            ...                 ...                  ...   \n",
       "6161         False          False               False                False   \n",
       "6162         False          False               False                False   \n",
       "6163         False          False               False                False   \n",
       "6164         False          False               False                False   \n",
       "6165         False          False               False                False   \n",
       "\n",
       "      interface_none  interface_ocpp  isDoS  \n",
       "0              False           False  False  \n",
       "1              False           False  False  \n",
       "2              False           False  False  \n",
       "3              False           False  False  \n",
       "4              False           False  False  \n",
       "...              ...             ...    ...  \n",
       "6161           False            True   True  \n",
       "6162           False            True   True  \n",
       "6163           False            True   True  \n",
       "6164           False            True   True  \n",
       "6165           False            True   True  \n",
       "\n",
       "[6166 rows x 42 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path.join('dataset', 'F:\\INTRUSION DETECTION DATASET\\EVSE-B-HPC-Kernel-Events-processed.csv'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eaefbc5-2330-4006-8593-d8cb764263d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>irq_softirq_exit</th>\n",
       "      <th>irq_softirq_entry</th>\n",
       "      <th>irq_softirq_raise</th>\n",
       "      <th>kmem_kmem_cache_free</th>\n",
       "      <th>kmem_kmem_cache_alloc</th>\n",
       "      <th>net_netif_rx</th>\n",
       "      <th>net_netif_rx_ni_exit</th>\n",
       "      <th>net_netif_rx_ni_entry</th>\n",
       "      <th>rpm_rpm_usage</th>\n",
       "      <th>rpm_rpm_resume</th>\n",
       "      <th>...</th>\n",
       "      <th>Scenario_DoS</th>\n",
       "      <th>Scenario_Recon</th>\n",
       "      <th>Label_attack</th>\n",
       "      <th>Label_benign</th>\n",
       "      <th>interface_any</th>\n",
       "      <th>interface_iso15118</th>\n",
       "      <th>interface_iso15118</th>\n",
       "      <th>interface_none</th>\n",
       "      <th>interface_ocpp</th>\n",
       "      <th>isDoS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.047937</td>\n",
       "      <td>0.047935</td>\n",
       "      <td>0.048078</td>\n",
       "      <td>0.010061</td>\n",
       "      <td>0.010228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042511</td>\n",
       "      <td>0.043414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.039543</td>\n",
       "      <td>0.039541</td>\n",
       "      <td>0.039677</td>\n",
       "      <td>0.030916</td>\n",
       "      <td>0.034589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013468</td>\n",
       "      <td>0.013822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.054762</td>\n",
       "      <td>0.054760</td>\n",
       "      <td>0.055019</td>\n",
       "      <td>0.041051</td>\n",
       "      <td>0.041990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.027018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.075644</td>\n",
       "      <td>0.075641</td>\n",
       "      <td>0.076153</td>\n",
       "      <td>0.040066</td>\n",
       "      <td>0.045505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049041</td>\n",
       "      <td>0.049565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.069371</td>\n",
       "      <td>0.069369</td>\n",
       "      <td>0.069576</td>\n",
       "      <td>0.040949</td>\n",
       "      <td>0.040037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047073</td>\n",
       "      <td>0.047478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6161</th>\n",
       "      <td>0.784351</td>\n",
       "      <td>0.784325</td>\n",
       "      <td>0.784332</td>\n",
       "      <td>0.277850</td>\n",
       "      <td>0.279868</td>\n",
       "      <td>0.706026</td>\n",
       "      <td>0.706005</td>\n",
       "      <td>0.705994</td>\n",
       "      <td>0.797535</td>\n",
       "      <td>0.797587</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6162</th>\n",
       "      <td>0.798481</td>\n",
       "      <td>0.798455</td>\n",
       "      <td>0.798658</td>\n",
       "      <td>0.248506</td>\n",
       "      <td>0.250153</td>\n",
       "      <td>0.601882</td>\n",
       "      <td>0.601860</td>\n",
       "      <td>0.601850</td>\n",
       "      <td>0.694036</td>\n",
       "      <td>0.694314</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6163</th>\n",
       "      <td>0.824827</td>\n",
       "      <td>0.824791</td>\n",
       "      <td>0.824802</td>\n",
       "      <td>0.265769</td>\n",
       "      <td>0.267416</td>\n",
       "      <td>0.656199</td>\n",
       "      <td>0.656210</td>\n",
       "      <td>0.656199</td>\n",
       "      <td>0.750532</td>\n",
       "      <td>0.750537</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6164</th>\n",
       "      <td>0.797086</td>\n",
       "      <td>0.797068</td>\n",
       "      <td>0.797098</td>\n",
       "      <td>0.251859</td>\n",
       "      <td>0.253625</td>\n",
       "      <td>0.618252</td>\n",
       "      <td>0.618262</td>\n",
       "      <td>0.618252</td>\n",
       "      <td>0.706262</td>\n",
       "      <td>0.706248</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6165</th>\n",
       "      <td>0.805959</td>\n",
       "      <td>0.805932</td>\n",
       "      <td>0.805937</td>\n",
       "      <td>0.259736</td>\n",
       "      <td>0.261380</td>\n",
       "      <td>0.644177</td>\n",
       "      <td>0.644187</td>\n",
       "      <td>0.644177</td>\n",
       "      <td>0.724908</td>\n",
       "      <td>0.724890</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6166 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      irq_softirq_exit  irq_softirq_entry  irq_softirq_raise  \\\n",
       "0             0.047937           0.047935           0.048078   \n",
       "1             0.039543           0.039541           0.039677   \n",
       "2             0.054762           0.054760           0.055019   \n",
       "3             0.075644           0.075641           0.076153   \n",
       "4             0.069371           0.069369           0.069576   \n",
       "...                ...                ...                ...   \n",
       "6161          0.784351           0.784325           0.784332   \n",
       "6162          0.798481           0.798455           0.798658   \n",
       "6163          0.824827           0.824791           0.824802   \n",
       "6164          0.797086           0.797068           0.797098   \n",
       "6165          0.805959           0.805932           0.805937   \n",
       "\n",
       "      kmem_kmem_cache_free  kmem_kmem_cache_alloc  net_netif_rx  \\\n",
       "0                 0.010061               0.010228      0.000000   \n",
       "1                 0.030916               0.034589      0.000000   \n",
       "2                 0.041051               0.041990      0.000000   \n",
       "3                 0.040066               0.045505      0.000000   \n",
       "4                 0.040949               0.040037      0.000000   \n",
       "...                    ...                    ...           ...   \n",
       "6161              0.277850               0.279868      0.706026   \n",
       "6162              0.248506               0.250153      0.601882   \n",
       "6163              0.265769               0.267416      0.656199   \n",
       "6164              0.251859               0.253625      0.618252   \n",
       "6165              0.259736               0.261380      0.644177   \n",
       "\n",
       "      net_netif_rx_ni_exit  net_netif_rx_ni_entry  rpm_rpm_usage  \\\n",
       "0                 0.000000               0.000000       0.042511   \n",
       "1                 0.000000               0.000000       0.013468   \n",
       "2                 0.000000               0.000000       0.026667   \n",
       "3                 0.000000               0.000000       0.049041   \n",
       "4                 0.000000               0.000000       0.047073   \n",
       "...                    ...                    ...            ...   \n",
       "6161              0.706005               0.705994       0.797535   \n",
       "6162              0.601860               0.601850       0.694036   \n",
       "6163              0.656210               0.656199       0.750532   \n",
       "6164              0.618262               0.618252       0.706262   \n",
       "6165              0.644187               0.644177       0.724908   \n",
       "\n",
       "      rpm_rpm_resume  ...  Scenario_DoS  Scenario_Recon  Label_attack  \\\n",
       "0           0.043414  ...           0.0             0.0           1.0   \n",
       "1           0.013822  ...           0.0             0.0           1.0   \n",
       "2           0.027018  ...           0.0             0.0           1.0   \n",
       "3           0.049565  ...           0.0             0.0           1.0   \n",
       "4           0.047478  ...           0.0             0.0           1.0   \n",
       "...              ...  ...           ...             ...           ...   \n",
       "6161        0.797587  ...           1.0             0.0           1.0   \n",
       "6162        0.694314  ...           1.0             0.0           1.0   \n",
       "6163        0.750537  ...           1.0             0.0           1.0   \n",
       "6164        0.706248  ...           1.0             0.0           1.0   \n",
       "6165        0.724890  ...           1.0             0.0           1.0   \n",
       "\n",
       "      Label_benign  interface_any  interface_iso15118  interface_iso15118   \\\n",
       "0              0.0            1.0                 0.0                  0.0   \n",
       "1              0.0            1.0                 0.0                  0.0   \n",
       "2              0.0            1.0                 0.0                  0.0   \n",
       "3              0.0            1.0                 0.0                  0.0   \n",
       "4              0.0            1.0                 0.0                  0.0   \n",
       "...            ...            ...                 ...                  ...   \n",
       "6161           0.0            0.0                 0.0                  0.0   \n",
       "6162           0.0            0.0                 0.0                  0.0   \n",
       "6163           0.0            0.0                 0.0                  0.0   \n",
       "6164           0.0            0.0                 0.0                  0.0   \n",
       "6165           0.0            0.0                 0.0                  0.0   \n",
       "\n",
       "      interface_none  interface_ocpp  isDoS  \n",
       "0                0.0             0.0    0.0  \n",
       "1                0.0             0.0    0.0  \n",
       "2                0.0             0.0    0.0  \n",
       "3                0.0             0.0    0.0  \n",
       "4                0.0             0.0    0.0  \n",
       "...              ...             ...    ...  \n",
       "6161             0.0             1.0    1.0  \n",
       "6162             0.0             1.0    1.0  \n",
       "6163             0.0             1.0    1.0  \n",
       "6164             0.0             1.0    1.0  \n",
       "6165             0.0             1.0    1.0  \n",
       "\n",
       "[6166 rows x 42 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min-max normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(df)\n",
    "df_normalized = pd.DataFrame(normalized_data, columns=df.columns)\n",
    "df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7cffebe-d04d-4683-8ac0-4d3aec248190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data splitting: training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and labels\n",
    "# X = df_normalized.iloc[:, :-1].values  # All columns except the last one\n",
    "X = df_normalized.iloc[:, :10].values  # All columns except the last one\n",
    "y = df_normalized.iloc[:, -1].values   # The last column (label)\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device).unsqueeze(1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device).unsqueeze(1)\n",
    "\n",
    "# If the labels are for classification, ensure they're of `long` dtype\n",
    "# y_train_tensor = y_train_tensor.long()\n",
    "# y_test_tensor = y_test_tensor.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f80c0913-f8f3-480c-8d01-7352cadea9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4932, 1, 10])\n",
      "torch.Size([1234, 1, 10])\n",
      "torch.Size([4932, 1])\n",
      "torch.Size([1234, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tensor.shape)\n",
    "print(X_test_tensor.shape)\n",
    "print(y_train_tensor.shape)\n",
    "print(y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67efae90-b4ea-4318-b723-73ba8d784e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBinaryClassifier(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(TransformerBinaryClassifier, self).__init__()\n",
    "        self.input_layer = nn.Linear(d_model, 64)   # Fix: input is feature size\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=64, nhead=2, dim_feedforward=128, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)  # add sequence dimension\n",
    "        x = self.input_layer(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)  # global average pooling\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e416298-b1b6-4375-88e9-8c315fde2a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4932, 10])\n",
      "torch.Size([1234, 10])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tensor.shape)\n",
    "print(X_test_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38b181a9-c888-447b-90f6-0621b6770549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WCGAN Epoch 10/100]  Critic Loss: -0.0014  Generator Loss: -0.0083\n",
      "[WCGAN Epoch 20/100]  Critic Loss: -0.0021  Generator Loss: -0.0072\n",
      "[WCGAN Epoch 30/100]  Critic Loss: -0.0017  Generator Loss: -0.0073\n",
      "[WCGAN Epoch 40/100]  Critic Loss: -0.0011  Generator Loss: -0.0078\n",
      "[WCGAN Epoch 50/100]  Critic Loss: -0.0007  Generator Loss: -0.0086\n",
      "[WCGAN Epoch 60/100]  Critic Loss: -0.0008  Generator Loss: -0.0072\n",
      "[WCGAN Epoch 70/100]  Critic Loss: -0.0008  Generator Loss: -0.0084\n",
      "[WCGAN Epoch 80/100]  Critic Loss: -0.0010  Generator Loss: -0.0074\n",
      "[WCGAN Epoch 90/100]  Critic Loss: -0.0008  Generator Loss: -0.0069\n",
      "[WCGAN Epoch 100/100]  Critic Loss: -0.0006  Generator Loss: -0.0083\n",
      "Epoch 1/100: Train Loss=0.6636, Val Loss=0.5821, Acc=0.9887, Prec=0.9415, Recall=0.9758\n",
      "Epoch 2/100: Train Loss=0.6084, Val Loss=0.4932, Acc=0.9903, Prec=0.9422, Recall=0.9879\n",
      "Epoch 3/100: Train Loss=0.5621, Val Loss=0.4056, Acc=0.9903, Prec=0.9422, Recall=0.9879\n",
      "Epoch 4/100: Train Loss=0.5163, Val Loss=0.3359, Acc=0.9911, Prec=0.9425, Recall=0.9939\n",
      "Epoch 5/100: Train Loss=0.4740, Val Loss=0.2927, Acc=0.9911, Prec=0.9425, Recall=0.9939\n",
      "Epoch 6/100: Train Loss=0.4422, Val Loss=0.2650, Acc=0.9911, Prec=0.9425, Recall=0.9939\n",
      "Epoch 7/100: Train Loss=0.4124, Val Loss=0.2444, Acc=0.9887, Prec=0.9266, Recall=0.9939\n",
      "Epoch 8/100: Train Loss=0.3919, Val Loss=0.2325, Acc=0.9846, Prec=0.9011, Recall=0.9939\n",
      "Epoch 9/100: Train Loss=0.3700, Val Loss=0.2161, Acc=0.9838, Prec=0.8962, Recall=0.9939\n",
      "Epoch 10/100: Train Loss=0.3509, Val Loss=0.2009, Acc=0.9838, Prec=0.8962, Recall=0.9939\n",
      "Epoch 11/100: Train Loss=0.3276, Val Loss=0.1901, Acc=0.9838, Prec=0.8962, Recall=0.9939\n",
      "Epoch 12/100: Train Loss=0.3058, Val Loss=0.1709, Acc=0.9838, Prec=0.8962, Recall=0.9939\n",
      "Epoch 13/100: Train Loss=0.2795, Val Loss=0.1618, Acc=0.9806, Prec=0.8770, Recall=0.9939\n",
      "Epoch 14/100: Train Loss=0.2571, Val Loss=0.1425, Acc=0.9797, Prec=0.8723, Recall=0.9939\n",
      "Epoch 15/100: Train Loss=0.2407, Val Loss=0.1324, Acc=0.9797, Prec=0.8723, Recall=0.9939\n",
      "Epoch 16/100: Train Loss=0.2255, Val Loss=0.1295, Acc=0.9797, Prec=0.8723, Recall=0.9939\n",
      "Epoch 17/100: Train Loss=0.2129, Val Loss=0.1262, Acc=0.9797, Prec=0.8723, Recall=0.9939\n",
      "Epoch 18/100: Train Loss=0.2065, Val Loss=0.1194, Acc=0.9789, Prec=0.8677, Recall=0.9939\n",
      "Epoch 19/100: Train Loss=0.1937, Val Loss=0.1092, Acc=0.9797, Prec=0.8723, Recall=0.9939\n",
      "Epoch 20/100: Train Loss=0.1896, Val Loss=0.1045, Acc=0.9797, Prec=0.8723, Recall=0.9939\n",
      "Epoch 21/100: Train Loss=0.1834, Val Loss=0.0990, Acc=0.9797, Prec=0.8723, Recall=0.9939\n",
      "Epoch 22/100: Train Loss=0.1776, Val Loss=0.1167, Acc=0.9765, Prec=0.8542, Recall=0.9939\n",
      "Epoch 23/100: Train Loss=0.1717, Val Loss=0.1050, Acc=0.9781, Prec=0.8632, Recall=0.9939\n",
      "Epoch 24/100: Train Loss=0.1696, Val Loss=0.1004, Acc=0.9781, Prec=0.8632, Recall=0.9939\n",
      "Epoch 25/100: Train Loss=0.1637, Val Loss=0.1062, Acc=0.9781, Prec=0.8632, Recall=0.9939\n",
      "Epoch 26/100: Train Loss=0.1621, Val Loss=0.0970, Acc=0.9781, Prec=0.8632, Recall=0.9939\n",
      "Epoch 27/100: Train Loss=0.1586, Val Loss=0.0995, Acc=0.9781, Prec=0.8632, Recall=0.9939\n",
      "Epoch 28/100: Train Loss=0.1541, Val Loss=0.0938, Acc=0.9781, Prec=0.8632, Recall=0.9939\n",
      "Epoch 29/100: Train Loss=0.1516, Val Loss=0.0902, Acc=0.9789, Prec=0.8677, Recall=0.9939\n",
      "Epoch 30/100: Train Loss=0.1530, Val Loss=0.0972, Acc=0.9781, Prec=0.8632, Recall=0.9939\n",
      "Epoch 31/100: Train Loss=0.1476, Val Loss=0.1026, Acc=0.9765, Prec=0.8542, Recall=0.9939\n",
      "Epoch 32/100: Train Loss=0.1439, Val Loss=0.1016, Acc=0.9765, Prec=0.8542, Recall=0.9939\n",
      "Epoch 33/100: Train Loss=0.1418, Val Loss=0.0929, Acc=0.9781, Prec=0.8632, Recall=0.9939\n",
      "Epoch 34/100: Train Loss=0.1395, Val Loss=0.0941, Acc=0.9781, Prec=0.8632, Recall=0.9939\n",
      "Early stopping triggered.\n",
      "✅ Training complete with WCGAN augmentation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ======================\n",
    "#  Define WCGAN Components\n",
    "# ======================\n",
    "\n",
    "# Generator (Conditional)\n",
    "class WCGAN_Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, condition_dim, output_dim):\n",
    "        super(WCGAN_Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim + condition_dim, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        x = torch.cat((noise, labels), dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Critic (Discriminator)\n",
    "class WCGAN_Critic(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim):\n",
    "        super(WCGAN_Critic, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + condition_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # Flatten if input is 3D\n",
    "        if x.dim() > 2:\n",
    "            x = x.view(x.size(0), -1)  # (batch_size, flattened_features)\n",
    "        d_in = torch.cat((x, labels), dim=1)\n",
    "        return self.model(d_in)\n",
    "\n",
    "\n",
    "\n",
    "# Wasserstein Loss\n",
    "def wasserstein_loss(y_pred, y_true):\n",
    "    return -torch.mean(y_pred * y_true)\n",
    "\n",
    "\n",
    "# ======================\n",
    "#  Train WCGAN\n",
    "# ======================\n",
    "\n",
    "def train_wcgan(X_train_tensor, y_train_tensor, noise_dim=32, n_epochs=100, batch_size=128, lr=1e-4, clip_value=0.01, n_critic=5):\n",
    "    device = X_train_tensor.device\n",
    "    input_dim = X_train_tensor.shape[1]\n",
    "    condition_dim = 1  # Label (binary)\n",
    "    \n",
    "    generator = WCGAN_Generator(noise_dim, condition_dim, input_dim).to(device)\n",
    "    critic = WCGAN_Critic(input_dim, condition_dim).to(device)\n",
    "\n",
    "    optimizer_G = optim.RMSprop(generator.parameters(), lr=lr)\n",
    "    optimizer_C = optim.RMSprop(critic.parameters(), lr=lr)\n",
    "\n",
    "    dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (real_samples, labels) in enumerate(loader):\n",
    "            real_samples, labels = real_samples.to(device), labels.to(device)\n",
    "\n",
    "            # ---------------------\n",
    "            # Train Critic\n",
    "            # ---------------------\n",
    "            for _ in range(n_critic):\n",
    "                optimizer_C.zero_grad()\n",
    "\n",
    "                # Real loss\n",
    "                real_validity = critic(real_samples, labels)\n",
    "                real_loss = torch.mean(real_validity)\n",
    "\n",
    "                # Fake samples\n",
    "                noise = torch.randn(real_samples.size(0), noise_dim, device=device)\n",
    "                fake_samples = generator(noise, labels)\n",
    "                fake_validity = critic(fake_samples.detach(), labels)\n",
    "                fake_loss = torch.mean(fake_validity)\n",
    "\n",
    "                # Wasserstein loss\n",
    "                loss_C = -(real_loss - fake_loss)\n",
    "                loss_C.backward()\n",
    "                optimizer_C.step()\n",
    "\n",
    "                # Weight clipping\n",
    "                for p in critic.parameters():\n",
    "                    p.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "            # ---------------------\n",
    "            # Train Generator\n",
    "            # ---------------------\n",
    "            optimizer_G.zero_grad()\n",
    "            noise = torch.randn(real_samples.size(0), noise_dim, device=device)\n",
    "            fake_samples = generator(noise, labels)\n",
    "            fake_validity = critic(fake_samples, labels)\n",
    "            loss_G = -torch.mean(fake_validity)\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"[WCGAN Epoch {epoch+1}/{n_epochs}]  Critic Loss: {loss_C:.4f}  Generator Loss: {loss_G:.4f}\")\n",
    "\n",
    "    return generator\n",
    "\n",
    "\n",
    "# ======================\n",
    "#  Generate Synthetic Data using WCGAN\n",
    "# ======================\n",
    "\n",
    "def generate_synthetic_data(generator, n_samples, noise_dim=32, label_value=1, device='cpu'):\n",
    "    labels = torch.full((n_samples, 1), label_value, device=device)\n",
    "    noise = torch.randn(n_samples, noise_dim, device=device)\n",
    "    synthetic_data = generator(noise, labels)\n",
    "    return synthetic_data.detach(), labels\n",
    "\n",
    "\n",
    "# ======================\n",
    "# Train Transformer Model (your existing code)\n",
    "# ======================\n",
    "\n",
    "# Example parameters\n",
    "d_model = 10\n",
    "nhead = 2\n",
    "num_layers = 2\n",
    "batch_size = 512\n",
    "epochs = 100\n",
    "patience = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerBinaryClassifier(d_model=X_train_tensor.shape[1]).to(device)\n",
    "\n",
    "# Define your TransformerBinaryClassifier (already built)\n",
    "# model = TransformerBinaryClassifier(d_model, nhead, num_layers).to(device)\n",
    "\n",
    "X_train_tensor = X_train_tensor.view(X_train_tensor.size(0), -1)\n",
    "X_test_tensor = X_test_tensor.view(X_test_tensor.size(0), -1)\n",
    "\n",
    "\n",
    "# Train WCGAN first\n",
    "generator = train_wcgan(X_train_tensor, y_train_tensor, n_epochs=100)\n",
    "\n",
    "# Generate synthetic samples\n",
    "synthetic_X, synthetic_y = generate_synthetic_data(generator, n_samples=2000, device=device)\n",
    "\n",
    "# Combine with real data\n",
    "X_augmented = torch.cat((X_train_tensor, synthetic_X), dim=0)\n",
    "y_augmented = torch.cat((y_train_tensor, synthetic_y), dim=0)\n",
    "\n",
    "# DataLoader for augmented dataset\n",
    "train_dataset = TensorDataset(X_augmented, y_augmented)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Loss and optimizer for Transformer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Early stopping setup\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# ======================\n",
    "#  Train Transformer on Augmented Data\n",
    "# ======================\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)  # Works now!\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            preds = (outputs > 0.5).long()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(test_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Acc={acc:.4f}, Prec={prec:.4f}, Recall={rec:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_model_augmented.pth\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "print(\"✅ Training complete with WCGAN augmentation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7be7b32f-8f3e-4753-b0be-0bd7855e28b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c696800-4729-4008-b809-4bf267bb9a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAGwCAYAAADWsX1oAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA2l0lEQVR4nO3de3wU9dn38e/mtEkwWQiQLIEAQVEOiYgBETyANydRFMpzF7lRb2yjoqCYAkJ54gG1JEIrRKEgUkpSEJFHG9RWkYOKIqIQQTmJVQMEIQZrTELIeef5g7LtEliy7IYlO5/36zWvdmd+M7k2pdlrr+v3m7EYhmEIAACYWpC/AwAAAP5HQgAAAEgIAAAACQEAABAJAQAAEAkBAAAQCQEAAJAU4u8AvOFwOHTkyBFFRUXJYrH4OxwAgIcMw1BZWZni4+MVFNR431ErKytVXV3t9XXCwsIUHh7ug4guPk06IThy5IgSEhL8HQYAwEsFBQVq165do1y7srJSiR0uUWFRndfXstvtys/PD8ikoEknBFFRUZKkg593VPQldD8QmP7Plb38HQLQaGqNGn1Ylev8e94YqqurVVhUp4N5HRUddf6fFaVlDnVIOaDq6moSgovNqTZB9CVBXv2PDFzMQixh/g4BaHQXou17SZRFl0Sd/89xKLBb0006IQAAoKHqDIfqvHh6T53h8F0wFyESAgCAKThkyKHzzwi8ObcpoM4OAACoEAAAzMEhh7wp+nt39sWPhAAAYAp1hqE64/zL/t6c2xTQMgAAAFQIAADmwKRC90gIAACm4JChOhKCs6JlAAAASAgAAOZwqmXgzeaJDz/8ULfddpvi4+NlsVi0Zs0al+OGYWjmzJmKj49XRESEBgwYoD179riMqaqq0sMPP6xWrVqpWbNmuv3223X48GGXMcXFxbr77rtls9lks9l099136+eff/b490NCAAAwhVOrDLzZPFFeXq4ePXpowYIFZzw+Z84czZ07VwsWLNC2bdtkt9s1ePBglZWVOcekpaUpNzdXq1at0ubNm3X8+HENHz5cdXX/flDT2LFjtXPnTq1du1Zr167Vzp07dffdd3v8+2EOAQAAHigtLXV5bbVaZbVa640bNmyYhg0bdsZrGIahrKwspaena9SoUZKknJwcxcXFaeXKlRo/frxKSkq0dOlSLV++XIMGDZIkrVixQgkJCdqwYYOGDh2qffv2ae3atdq6dav69OkjSVqyZIn69u2r/fv364orrmjw+6JCAAAwBYcPNklKSEhwludtNpsyMzM9jiU/P1+FhYUaMmSIc5/ValX//v21ZcsWSVJeXp5qampcxsTHxyspKck55pNPPpHNZnMmA5J07bXXymazOcc0FBUCAIAp1Hm5yuDUuQUFBYqOjnbuP1N14FwKCwslSXFxcS774+LidPDgQeeYsLAwtWjRot6YU+cXFhYqNja23vVjY2OdYxqKhAAAYAp1hrx82uHJ/4yOjnZJCLxx+mOfDcM456OgTx9zpvENuc7paBkAAHCB2e12Sar3Lb6oqMhZNbDb7aqurlZxcbHbMT/88EO96x87dqxe9eFcSAgAAKbgqzkEvpCYmCi73a7169c791VXV2vTpk3q16+fJCklJUWhoaEuY44ePardu3c7x/Tt21clJSX67LPPnGM+/fRTlZSUOMc0FC0DAIApOGRRnTwro59+vieOHz+ub775xvk6Pz9fO3fuVExMjNq3b6+0tDRlZGSoc+fO6ty5szIyMhQZGamxY8dKkmw2m1JTUzVlyhS1bNlSMTExmjp1qpKTk52rDrp27aqbb75Z9913nxYvXixJuv/++zV8+HCPVhhIJAQAADSK7du366abbnK+njx5siRp3Lhxys7O1rRp01RRUaEJEyaouLhYffr00bp16xQVFeU8Z968eQoJCdHo0aNVUVGhgQMHKjs7W8HBwc4xL7/8siZNmuRcjXD77bef9d4H7lgMo+k+z7G0tFQ2m03FX3dSdBTdDwSmYZ2u9XcIQKOpNar1XuVqlZSU+Gyi3ulOfVZs3xOnS7z4rDhe5lCv7j80aqz+RIUAAGAKdV62DLw5tyngazUAAKBCAAAwByoE7pEQAABMwWFY5DC8WGXgxblNAS0DAABAhQAAYA60DNwjIQAAmEKdglTnRWG8zoexXIxICAAApmB4OYfAYA4BAAAIdFQIAACmwBwC90gIAACmUGcEqc7wYg5Bk73Rf8PQMgAAAFQIAADm4JBFDi++BzsU2CUCEgIAgCkwh8A9WgYAAIAKAQDAHLyfVEjLAACAJu/kHAIvHm5EywAAAAQ6KgQAAFNwePksA1YZAAAQAJhD4B4JAQDAFBwK4j4EbjCHAAAAUCEAAJhDnWFRnRePMPbm3KaAhAAAYAp1Xk4qrKNlAAAAAh0VAgCAKTiMIDm8WGXgYJUBAABNHy0D92gZAAAAKgQAAHNwyLuVAg7fhXJRIiEAAJiC9zcmCuyiemC/OwAA0CBUCAAApuD9swwC+zs0CQEAwBQcssghb+YQcKdCAACaPCoE7gX2uwMAAA1ChQAAYAre35gosL9DkxAAAEzBYVjk8OY+BAH+tMPATncAAECDUCEAAJiCw8uWQaDfmIiEAABgCt4/7TCwE4LAfncAAKBBqBAAAEyhThbVeXFzIW/ObQpICAAApkDLwL3AfncAAKBBqBAAAEyhTt6V/et8F8pFiYQAAGAKtAzcIyEAAJgCDzdyL7DfHQAAaBAqBAAAUzBkkcOLOQQGyw4BAGj6aBm4F9jvDgAANAgVAgCAKfD4Y/dICAAAplDn5dMOvTm3KQjsdwcAABqECgEAwBRoGbhHQgAAMAWHguTwojDuzblNQWC/OwAA0CBUCAAAplBnWFTnRdnfm3ObAhICAIApMIfAPRICAIApGF4+7dDgToUAAMBTtbW1euyxx5SYmKiIiAh16tRJTz/9tBwOh3OMYRiaOXOm4uPjFRERoQEDBmjPnj0u16mqqtLDDz+sVq1aqVmzZrr99tt1+PBhn8dLQgAAMIU6WbzePDF79my9+OKLWrBggfbt26c5c+bo97//vebPn+8cM2fOHM2dO1cLFizQtm3bZLfbNXjwYJWVlTnHpKWlKTc3V6tWrdLmzZt1/PhxDR8+XHV1dT773Ui0DAAAJuEwvJsH4DBO/mdpaanLfqvVKqvVWm/8J598ohEjRujWW2+VJHXs2FGvvPKKtm/fLulkdSArK0vp6ekaNWqUJCknJ0dxcXFauXKlxo8fr5KSEi1dulTLly/XoEGDJEkrVqxQQkKCNmzYoKFDh573+zkdFQIAADyQkJAgm83m3DIzM8847vrrr9fGjRv19ddfS5K++OILbd68WbfccoskKT8/X4WFhRoyZIjzHKvVqv79+2vLli2SpLy8PNXU1LiMiY+PV1JSknOMr1AhMJldW5vp/y2M1T92ReqnH0L15NJ89RtW4jxuGNKK5+x6++WWOl4SrC49T2hixmF1vKKy3rUMQ3rsrk7a/n50ves8OS5R3+6J0M//DFGUrU49byhTavoRtbTXXpD3CZzN6Ae/13VDi9WuU4WqK4O09/Mo/Xl2gr7Pj3AZl3BphX49/ZCS+5TJYjF06B8Ryni4s44dqf9NEE2Dw8tJhafOLSgoUHR0tHP/maoDkjR9+nSVlJSoS5cuCg4OVl1dnWbNmqX/+Z//kSQVFhZKkuLi4lzOi4uL08GDB51jwsLC1KJFi3pjTp3vKyQEJlN5IkiduldoyJif9My9ifWOr/5jrP76UmtNyTqkdp2qtDIrTjPGXKqlH+1T5CUOl7G5S1rLcpbqW4/rjmvMpB8UE1ejH4+GasnTbfXMfYnKeusfjfG2gAZLvqZMby2P09dfNlNwsKFxUw9r1l++0vghV6qqIliS1KZ9pf6weq/eXd1aK7LaqbwsWAmXVai6iqJqU+aQRQ4P5wGcfr4kRUdHuyQEZ/Pqq69qxYoVWrlypbp3766dO3cqLS1N8fHxGjdunHOc5bQ/pIZh1Nt3uoaM8ZTf/3UvXLhQiYmJCg8PV0pKij766CN/hxTQev9Xme6ZXqjrbympd8wwpDV/aq0xk37Q9beUqGOXSk19/pCqKoL0fq5rdvrtnnC9vri1Js89dMafM+r+Y+qackJx7WrUvfcJ3fHQD/rq80jV1jTK2wIa7PFfddGG11vr0D8ilf9VM82b1klxbavVOancOWbclAJt+8CmP89ur2/3NlNhQbi2vd9CJf8M9WPkaGoeffRR/fa3v9WYMWOUnJysu+++W7/5zW+cLQa73S5J9b7pFxUVOasGdrtd1dXVKi4uPusYX/FrQvDqq68qLS1N6enp2rFjh2644QYNGzZMhw6d+UMGjavwUJh+KgpVSv9/z24NsxpKvva49m5v5txXecKiZyd01MRZhxUTe+4WQGlxsN77awt161WuEP6e4iITGXVypnZZycmCqcViqPdNP+v7/Aj9LvsrvfJZnub9dbf6Dv7Jn2HCB07dqdCbzRMnTpxQUJDrx2xwcLBz2WFiYqLsdrvWr1/vPF5dXa1NmzapX79+kqSUlBSFhoa6jDl69Kh2797tHOMrfk0I5s6dq9TUVN17773q2rWrsrKylJCQoEWLFvkzLNP6qejkH8QWrV2/xrdoXaPion93lxbPbKtuvcrV72bXmban+9Pv2uj2S5P1y+7JOnYkTDOX5fs+aMArhu5PP6jd26J08OtISVLzljWKvMSh0Q8c0fYPbUof10Vb1sXosUX/UPI17v/N4+J2ag6BN5snbrvtNs2aNUt///vfdeDAAeXm5mru3Ln6xS9+IelkqyAtLU0ZGRnKzc3V7t27dc899ygyMlJjx46VJNlsNqWmpmrKlCnauHGjduzYobvuukvJycnOVQe+4rc5BNXV1crLy9Nvf/tbl/1Dhgw568zJqqoqVVVVOV+fvvQDPnJaEmwYFue+T96N1s6Po7Rw3f5zXuaXDxbp5v/5ST8cDtXLc+36/SPt9fRf8s867wC40CY8dUCJXU5o6uhuzn2Wf/3N/2RDC635cxtJ0nf7mqnb1WW65c4i7frs3L1jQJLmz5+vxx9/XBMmTFBRUZHi4+M1fvx4PfHEE84x06ZNU0VFhSZMmKDi4mL16dNH69atU1RUlHPMvHnzFBISotGjR6uiokIDBw5Udna2goODfRqv3xKCH3/8UXV1dWecXXm2mZOZmZl66qmnLkR4pnSq/F9cFKqWcf9uBfz8Y4hatD75eufHUTp6IEyjuiS7nPvMfR2V1Kdcv3/9G+c+W8s62VrWqd2lVWrf+aDu6tVd+/Ii1a3XiQvwbgD3HnzygK4d+LMeHdNVPxb+e5Z4aXGIamssOvQP11UHBd9GqFtK2emXQRPikJfPMvBwQmJUVJSysrKUlZV11jEWi0UzZ87UzJkzzzomPDxc8+fPd7mhUWPw+yoDT2ZXzpgxQ5MnT3a+Li0tVUJCQqPGZyb29tWKia3R5x9G6bLkCklSTbVFu7ZeotT0I5KkOx76QcPG/tPlvPH/1UXjZ36va4ecvWJj/OuGHjXVfp/HCtMz9ODMg+o35CdNH9tNPxwOdzlaWxOkr79spnadKlz2t+1YqSKWHDZphperDAwvzm0K/JYQtGrVSsHBwW5nV57ubHeDQsNVlAfpSP6/f4eFBWH6dneEoprXKrZdjUbee0yr5sepbacqtU2s0isvxMka4dBNvzg5wzUmtvaMEwlj29bI3r5akvTVjkjt3xGppGvKdUnzWh09aNVffm9Xm45V6ppSXu9c4EKa+PQBDbj9n3r6/stVcTxILVqd/HdbXhbiXFb4+pI2+u0L32j3Z0X6Ymu0et34s/oMLNb0sd3cXRoXOZ526J7fEoKwsDClpKRo/fr1zgkWkrR+/XqNGDHCX2EFvK+/iNS0/77M+XrxzLaSpMGjf9LUrEMaPbFI1ZVBWjCjncr+dWOizFe+rXcPAnes4Q59/I5Ny5+zq/JEkGJia9TrpjL930UHFWY1fP6eAE8Mv6tIkjRn1T6X/c892kkbXm8tSdqyLkYLHu+o0Q8e0QNPHtDh7yL0uwmdtWd7VL3rAYHCYhiG3/5Cv/rqq7r77rv14osvqm/fvnrppZe0ZMkS7dmzRx06dDjn+aWlpbLZbCr+upOioyhFIzAN63Stv0MAGk2tUa33KlerpKSkQTf7OR+nPit+sf5XCm0Wdt7XqSmvVu7gZY0aqz/5dQ7BHXfcoX/+8596+umndfToUSUlJentt99uUDIAAIAnaBm45/dJhRMmTNCECRP8HQYAAKbm94QAAIALwVfPMghUJAQAAFOgZeAeM/EAAAAVAgCAOVAhcI+EAABgCiQE7tEyAAAAVAgAAOZAhcA9EgIAgCkY8m7pYKDfeJ2EAABgClQI3GMOAQAAoEIAADAHKgTukRAAAEyBhMA9WgYAAIAKAQDAHKgQuEdCAAAwBcOwyPDiQ92bc5sCWgYAAIAKAQDAHByyeHVjIm/ObQpICAAApsAcAvdoGQAAACoEAABzYFKheyQEAABToGXgHgkBAMAUqBC4xxwCAABAhQAAYA6Gly2DQK8QkBAAAEzBkGQY3p0fyGgZAAAAKgQAAHNwyCILdyo8KxICAIApsMrAPVoGAACACgEAwBwchkUWbkx0ViQEAABTMAwvVxkE+DIDWgYAAIAKAQDAHJhU6B4JAQDAFEgI3CMhAACYApMK3WMOAQAAoEIAADAHVhm4R0IAADCFkwmBN3MIfBjMRYiWAQAAoEIAADAHVhm4R0IAADAF41+bN+cHMloGAACACgEAwBxoGbhHQgAAMAd6Bm6REAAAzMHLCoECvELAHAIAAECFAABgDtyp0D0SAgCAKTCp0D1aBgAAgAoBAMAkDIt3EwMDvEJAQgAAMAXmELhHywAAAFAhAACYBDcmcouEAABgCqwycK9BCcELL7zQ4AtOmjTpvIMBACCQfP/995o+fbreeecdVVRU6PLLL9fSpUuVkpIiSTIMQ0899ZReeuklFRcXq0+fPvrjH/+o7t27O69RVVWlqVOn6pVXXlFFRYUGDhyohQsXql27dj6NtUEJwbx58xp0MYvFQkIAALh4XcCyf3Fxsa677jrddNNNeueddxQbG6tvv/1WzZs3d46ZM2eO5s6dq+zsbF1++eX63e9+p8GDB2v//v2KioqSJKWlpemtt97SqlWr1LJlS02ZMkXDhw9XXl6egoODfRZvgxKC/Px8n/1AAAD8wVctg9LSUpf9VqtVVqu13vjZs2crISFBy5Ytc+7r2LHjf1zPUFZWltLT0zVq1ChJUk5OjuLi4rRy5UqNHz9eJSUlWrp0qZYvX65BgwZJklasWKGEhARt2LBBQ4cOPe/3c7rzXmVQXV2t/fv3q7a21mfBAADQaAwfbJISEhJks9mcW2Zm5hl/3JtvvqlevXrpl7/8pWJjY9WzZ08tWbLEeTw/P1+FhYUaMmSIc5/ValX//v21ZcsWSVJeXp5qampcxsTHxyspKck5xlc8TghOnDih1NRURUZGqnv37jp06JCkk3MHnn32WZ8GBwDAxaagoEAlJSXObcaMGWcc991332nRokXq3Lmz3n33XT3wwAOaNGmS/vKXv0iSCgsLJUlxcXEu58XFxTmPFRYWKiwsTC1atDjrGF/xOCGYMWOGvvjiC33wwQcKDw937h80aJBeffVVnwYHAIDvWHywSdHR0S7bmdoFkuRwOHT11VcrIyNDPXv21Pjx43Xfffdp0aJFrlFZXNsYhmHU23e6hozxlMcJwZo1a7RgwQJdf/31LsF069ZN3377rU+DAwDAZ3zUMmioNm3aqFu3bi77unbt6qys2+12Sar3Tb+oqMhZNbDb7aqurlZxcfFZx/iKxwnBsWPHFBsbW29/eXm5z7MVAACaquuuu0779+932ff111+rQ4cOkqTExETZ7XatX7/eeby6ulqbNm1Sv379JEkpKSkKDQ11GXP06FHt3r3bOcZXPE4Ievfurb///e/O16eSgCVLlqhv376+iwwAAF+6wBWC3/zmN9q6dasyMjL0zTffaOXKlXrppZc0ceJESSc/P9PS0pSRkaHc3Fzt3r1b99xzjyIjIzV27FhJks1mU2pqqqZMmaKNGzdqx44duuuuu5ScnOxcdeArHt+pMDMzUzfffLP27t2r2tpaPf/889qzZ48++eQTbdq0yafBAQDgMxf4aYe9e/dWbm6uZsyYoaefflqJiYnKysrSnXfe6Rwzbdo0VVRUaMKECc4bE61bt855DwLp5L2AQkJCNHr0aOeNibKzs316DwJJshiG589v2rVrl/7whz8oLy/POWli+vTpSk5O9mlw51JaWiqbzabirzspOornNCEwDet0rb9DABpNrVGt9ypXq6SkRNHR0Y3yM059ViT88SkFRYSf+4SzcFRUqmDik40aqz+d17MMkpOTlZOT4+tYAABoNDz+2L3zSgjq6uqUm5urffv2yWKxqGvXrhoxYoRCQnhWEgDgIsXTDt3y+BN89+7dGjFihAoLC3XFFVdIOjlrsnXr1nrzzTcveNsAAAB4z+PG+7333qvu3bvr8OHD+vzzz/X555+roKBAV155pe6///7GiBEAAO+dmlTozRbAPK4QfPHFF9q+fbvLbRRbtGihWbNmqXfv3j4NDgAAX7EYJzdvzg9kHlcIrrjiCv3www/19hcVFemyyy7zSVAAAPjcBb4PQVPToISgtLTUuWVkZGjSpEl67bXXdPjwYR0+fFivvfaa0tLSNHv27MaOFwAANIIGtQyaN2/ucltiwzA0evRo575TtzK47bbbVFdX1whhAgDgpQt8Y6KmpkEJwfvvv9/YcQAA0LhYduhWgxKC/v37N3YcAADAj877TkInTpzQoUOHVF1d7bL/yiuv9DooAAB8jgqBWx4nBMeOHdOvfvUrvfPOO2c8zhwCAMBFiYTALY+XHaalpam4uFhbt25VRESE1q5dq5ycHHXu3FlvvvlmY8QIAAAamccVgvfee09vvPGGevfuraCgIHXo0EGDBw9WdHS0MjMzdeuttzZGnAAAeIdVBm55XCEoLy9XbGysJCkmJkbHjh2TdPIJiJ9//rlvowMAwEdO3anQmy2QndedCvfv3y9Juuqqq7R48WJ9//33evHFF9WmTRufBwgAABqfxy2DtLQ0HT16VJL05JNPaujQoXr55ZcVFham7OxsX8cHAIBvMKnQLY8TgjvvvNP533v27KkDBw7oq6++Uvv27dWqVSufBgcAAC6M874PwSmRkZG6+uqrfRELAACNxiIvn3bos0guTg1KCCZPntzgC86dO/e8gwEAAP7RoIRgx44dDbrYfz4A6UL6xeXJCrGE+uVnA43N0ruzv0MAGo1RVynlXagfxrJDd3i4EQDAHJhU6JbHyw4BAEDg8XpSIQAATQIVArdICAAApuDt3Qa5UyEAAAh4VAgAAOZAy8Ct86oQLF++XNddd53i4+N18OBBSVJWVpbeeOMNnwYHAIDPGD7YApjHCcGiRYs0efJk3XLLLfr5559VV1cnSWrevLmysrJ8HR8AALgAPE4I5s+fryVLlig9PV3BwcHO/b169dKuXbt8GhwAAL7C44/d83gOQX5+vnr27Flvv9VqVXl5uU+CAgDA57hToVseVwgSExO1c+fOevvfeecddevWzRcxAQDge8whcMvjCsGjjz6qiRMnqrKyUoZh6LPPPtMrr7yizMxM/elPf2qMGAEAQCPzOCH41a9+pdraWk2bNk0nTpzQ2LFj1bZtWz3//PMaM2ZMY8QIAIDXuDGRe+d1H4L77rtP9913n3788Uc5HA7Fxsb6Oi4AAHyL+xC45dWNiVq1auWrOAAAgB95nBAkJibKYjn7TMvvvvvOq4AAAGgU3i4dpELgKi0tzeV1TU2NduzYobVr1+rRRx/1VVwAAPgWLQO3PE4IHnnkkTPu/+Mf/6jt27d7HRAAALjwfPa0w2HDhun111/31eUAAPAt7kPgls+edvjaa68pJibGV5cDAMCnWHbonscJQc+ePV0mFRqGocLCQh07dkwLFy70aXAAAODC8DghGDlypMvroKAgtW7dWgMGDFCXLl18FRcAALiAPEoIamtr1bFjRw0dOlR2u72xYgIAwPdYZeCWR5MKQ0JC9OCDD6qqqqqx4gEAoFHw+GP3PF5l0KdPH+3YsaMxYgEAAH7i8RyCCRMmaMqUKTp8+LBSUlLUrFkzl+NXXnmlz4IDAMCnAvxbvjcanBD8+te/VlZWlu644w5J0qRJk5zHLBaLDMOQxWJRXV2d76MEAMBbzCFwq8EJQU5Ojp599lnl5+c3ZjwAAMAPGpwQGMbJ1KhDhw6NFgwAAI2FGxO559EcAndPOQQA4KJGy8AtjxKCyy+//JxJwU8//eRVQAAA4MLzKCF46qmnZLPZGisWAAAaDS0D9zxKCMaMGaPY2NjGigUAgMZDy8CtBt+YiPkDAAAELo9XGQAA0CRRIXCrwQmBw+FozDgAAGhUzCFwz+NbFwMA0CRRIXDL44cbAQCAwEOFAABgDlQI3KJCAAAwhVNzCLzZzldmZqYsFovS0tKc+wzD0MyZMxUfH6+IiAgNGDBAe/bscTmvqqpKDz/8sFq1aqVmzZrp9ttv1+HDh88/EDdICAAAaETbtm3TSy+9pCuvvNJl/5w5czR37lwtWLBA27Ztk91u1+DBg1VWVuYck5aWptzcXK1atUqbN2/W8ePHNXz48EZ5sjAJAQDAHAwfbJJKS0tdtqqqqrP+yOPHj+vOO+/UkiVL1KJFi3+HYhjKyspSenq6Ro0apaSkJOXk5OjEiRNauXKlJKmkpERLly7Vc889p0GDBqlnz55asWKFdu3apQ0bNvj0VyOREAAATMJXLYOEhATZbDbnlpmZedafOXHiRN16660aNGiQy/78/HwVFhZqyJAhzn1Wq1X9+/fXli1bJEl5eXmqqalxGRMfH6+kpCTnGF9iUiEAAB4oKChQdHS087XVaj3juFWrVunzzz/Xtm3b6h0rLCyUJMXFxbnsj4uL08GDB51jwsLCXCoLp8acOt+XSAgAAObgo1UG0dHRLgnBmRQUFOiRRx7RunXrFB4eftZxpz8WwDCMcz4qoCFjzgctAwCAOfhoDkFD5OXlqaioSCkpKQoJCVFISIg2bdqkF154QSEhIc7KwOnf9IuKipzH7Ha7qqurVVxcfNYxvkRCAACAjw0cOFC7du3Szp07nVuvXr105513aufOnerUqZPsdrvWr1/vPKe6ulqbNm1Sv379JEkpKSkKDQ11GXP06FHt3r3bOcaXaBkAAEzB8q/Nm/MbKioqSklJSS77mjVrppYtWzr3p6WlKSMjQ507d1bnzp2VkZGhyMhIjR07VpJks9mUmpqqKVOmqGXLloqJidHUqVOVnJxcb5KiL5AQAADM4SK7U+G0adNUUVGhCRMmqLi4WH369NG6desUFRXlHDNv3jyFhIRo9OjRqqio0MCBA5Wdna3g4GDfBiPJYjTh5xqXlpbKZrNpgEYoxBLq73CARmHpnezvEIBGU1tXqffzMlVSUnLOiXrn69RnRfcHMhRsPfsEv3Opq6rUnhf/b6PG6k/MIQAAALQMAAAmcZG1DC42JAQAAPMI8A91b9AyAAAAVAgAAObg7SOMvTm3KSAhAACYA3MI3KJlAAAAqBAAAMyBloF7JAQAAHOgZeAWLQMAAECFAABgDrQM3CMhAACYAy0Dt0gIAADmQELgFnMIAAAAFQIAgDkwh8A9EgIAgDnQMnCLlgEAAKBCAAAwB4thyGKc/9d8b85tCkgIAADmQMvALVoGAACACgEAwBxYZeAeCQEAwBxoGbhFywAAAFAhAACYAy0D90gIAADmQMvALRICAIApUCFwjzkEAACACgEAwCRoGbhFQgAAMI1AL/t7g5YBAACgQgAAMAnDOLl5c34AIyEAAJgCqwzco2UAAACoEAAATIJVBm6REAAATMHiOLl5c34go2UAAACoEKBhkvoc1y8nHFPn5BNqaa/VzF931Cdrbf4OC2iQpG4/6L9/sVedL/tJLWMq9FRGf33yaYLLmIR2JUod97mSuxfJEmTo4KHmyphzg4792Oy0qxl65on31TvlyBmvg4sYLQO3SAjQIOGRDn23J1zrVrXQE0sP+jscwCPh4bXKP9BC6zdeqsdnfFjveBt7mZ7LfFfvbrhMy1f2UPmJULVvV6LqmuB6Y39x+1eBvvosYLHKwD2/tgw+/PBD3XbbbYqPj5fFYtGaNWv8GQ7c2P5+tHLmtNHH7zT3dyiAx7Z/3lY5L1+lj7e2P+PxcXft1La8tlqac7W+zY9R4Q9R+iyvnUpKwl3GJXYs1qgR+zRvft8LETZ87dR9CLzZAphfE4Ly8nL16NFDCxYs8GcYAEzMYjF0Ta/v9f2RKM2auVGrcv6fsn7/jvr2KXAZZw2r1W+nbtYfF/dW8c8RfooWaDx+bRkMGzZMw4YNa/D4qqoqVVVVOV+XlpY2RlgATKS5rVKREbUa/X/2KOflq7Q0p6d6XX1Ej/92k6Y/Nli79sRJksanbte+r1pp62fMGWiqaBm416RWGWRmZspmszm3hAT+jwnAO5agk3/lP/k0QblvdtV3+TFa/XqSPtveVrfe/LUk6dprCtTjyh/04p96+TNUeMvwwRbAmtSkwhkzZmjy5MnO16WlpSQFALxSWmpVba1FhwpcV80cKrCpe7djkqQeyT+ojb1Mr69c7TLmsekfas/e1pr22JALFi/QWJpUQmC1WmW1Wv0dBoAAUlsbrK+/aal2bV1bkG3blqmo6OSSw9Wvd9fa9Ze5HF88/2966c8p2vpZuwsWK7xDy8C9JpUQwH/CI+sUn1jtfG1PqFan7hUq+zlYx74P82NkwLmFh9covk2Z87U97rg6Jf6ksjKrjv3YTK/ldtOMqZu1a0+svthlV6+rj+ja3oc1LX2wJKn454gzTiQsOtZMPxRdcsHeB7zE0w7dIiFAg1zeo0K/f/1b5+sHnjoiSVr3ags995szL+UCLhaXX/ZPzZm1wfl6fGqeJGn9xk567oV+2rK1veYvukZ3/PcePXjfdh3+PlrPPHuj9uyL9VfIwAXn14Tg+PHj+uabb5yv8/PztXPnTsXExKh9ez5kLiZffnKJhsb38HcYwHn5crddN4+4y+2YdRsv07qNl7kd85/OdT1cfGgZuOfXhGD79u266aabnK9PTRgcN26csrOz/RQVACAgcetit/yaEAwYMEBGgPdkAABoCphDAAAwBVoG7pEQAADMwWGc3Lw5P4CREAAAzIE5BG41qVsXAwCAxkGFAABgChZ5OYfAZ5FcnEgIAADmwJ0K3aJlAAAAqBAAAMyBZYfukRAAAMyBVQZu0TIAAABUCAAA5mAxDFm8mBjozblNARUCAIA5OHyweSAzM1O9e/dWVFSUYmNjNXLkSO3fv99ljGEYmjlzpuLj4xUREaEBAwZoz549LmOqqqr08MMPq1WrVmrWrJluv/12HT582NN3f04kBAAANIJNmzZp4sSJ2rp1q9avX6/a2loNGTJE5eXlzjFz5szR3LlztWDBAm3btk12u12DBw9WWVmZc0xaWppyc3O1atUqbd68WcePH9fw4cNVV1fn03hpGQAATMFXLYPS0lKX/VarVVartd74tWvXurxetmyZYmNjlZeXpxtvvFGGYSgrK0vp6ekaNWqUJCknJ0dxcXFauXKlxo8fr5KSEi1dulTLly/XoEGDJEkrVqxQQkKCNmzYoKFDh573+zkdFQIAgDkYPtgkJSQkyGazObfMzMwG/fiSkhJJUkxMjCQpPz9fhYWFGjJkiHOM1WpV//79tWXLFklSXl6eampqXMbEx8crKSnJOcZXqBAAAMzBR3cqLCgoUHR0tHP3maoD9U81NHnyZF1//fVKSkqSJBUWFkqS4uLiXMbGxcXp4MGDzjFhYWFq0aJFvTGnzvcVEgIAADwQHR3tkhA0xEMPPaQvv/xSmzdvrnfMYnF9SoJhGPX2na4hYzxFywAAYAqn7lTozXY+Hn74Yb355pt6//331a5dO+d+u90uSfW+6RcVFTmrBna7XdXV1SouLj7rGF8hIQAAmMOploE3m0c/ztBDDz2kv/71r3rvvfeUmJjocjwxMVF2u13r16937quurtamTZvUr18/SVJKSopCQ0Ndxhw9elS7d+92jvEVWgYAADSCiRMnauXKlXrjjTcUFRXlrATYbDZFRETIYrEoLS1NGRkZ6ty5szp37qyMjAxFRkZq7NixzrGpqamaMmWKWrZsqZiYGE2dOlXJycnOVQe+QkIAADAFi+Pk5s35nli0aJEkacCAAS77ly1bpnvuuUeSNG3aNFVUVGjChAkqLi5Wnz59tG7dOkVFRTnHz5s3TyEhIRo9erQqKio0cOBAZWdnKzg4+PzfzBlYDKPp3ouxtLRUNptNAzRCIZZQf4cDNApL72R/hwA0mtq6Sr2fl6mSkhKPJ+o1lPOz4pp0hYSEn/d1amsr9cFnsxo1Vn9iDgEAAKBlAAAwCR5/7BYJAQDAFHjaoXu0DAAAABUCAIBJ+OjWxYGKhAAAYA6GJC+WHTKHAACAAMAcAveYQwAAAKgQAABMwpCXcwh8FslFiYQAAGAOTCp0i5YBAACgQgAAMAmHJIuX5wcwEgIAgCmwysA9WgYAAIAKAQDAJJhU6BYJAQDAHEgI3KJlAAAAqBAAAEyCCoFbJAQAAHNg2aFbJAQAAFNg2aF7zCEAAABUCAAAJsEcArdICAAA5uAwJIsXH+qOwE4IaBkAAAAqBAAAk6Bl4BYJAQDAJLxMCBTYCQEtAwAAQIUAAGAStAzcIiEAAJiDw5BXZX9WGQAAgEBHhQAAYA6G4+TmzfkBjIQAAGAOzCFwi4QAAGAOzCFwizkEAACACgEAwCRoGbhFQgAAMAdDXiYEPovkokTLAAAAUCEAAJgELQO3SAgAAObgcEjy4l4CjsC+DwEtAwAAQIUAAGAStAzcIiEAAJgDCYFbtAwAAAAVAgCASXDrYrdICAAApmAYDhlePLHQm3ObAhICAIA5GIZ33/KZQwAAAAIdFQIAgDkYXs4hCPAKAQkBAMAcHA7J4sU8gACfQ0DLAAAAUCEAAJgELQO3SAgAAKZgOBwyvGgZBPqyQ1oGAACACgEAwCRoGbhFQgAAMAeHIVlICM6GlgEAAKBCAAAwCcOQ5M19CAK7QkBCAAAwBcNhyPCiZWCQEAAAEAAMh7yrELDsEAAABDgqBAAAU6Bl4B4JAQDAHGgZuNWkE4JT2Vqtary61wRwMbPUVfo7BKDR1NZVSbow3769/ayoVY3vgrkINemEoKysTJK0WW/7ORKgEeW94e8IgEZXVlYmm83WKNcOCwuT3W7X5kLvPyvsdrvCwsJ8ENXFx2I04aaIw+HQkSNHFBUVJYvF4u9wTKG0tFQJCQkqKChQdHS0v8MBfIp/3xeeYRgqKytTfHy8goIab557ZWWlqqurvb5OWFiYwsPDfRDRxadJVwiCgoLUrl07f4dhStHR0fzBRMDi3/eF1ViVgf8UHh4esB/kvsKyQwAAQEIAAABICOAhq9WqJ598Ular1d+hAD7Hv2+YWZOeVAgAAHyDCgEAACAhAAAAJAQAAEAkBAAAQCQE8MDChQuVmJio8PBwpaSk6KOPPvJ3SIBPfPjhh7rtttsUHx8vi8WiNWvW+Dsk4IIjIUCDvPrqq0pLS1N6erp27NihG264QcOGDdOhQ4f8HRrgtfLycvXo0UMLFizwdyiA37DsEA3Sp08fXX311Vq0aJFzX9euXTVy5EhlZmb6MTLAtywWi3JzczVy5Eh/hwJcUFQIcE7V1dXKy8vTkCFDXPYPGTJEW7Zs8VNUAABfIiHAOf3444+qq6tTXFycy/64uDgVFhb6KSoAgC+REKDBTn/EtGEYPHYaAAIECQHOqVWrVgoODq5XDSgqKqpXNQAANE0kBDinsLAwpaSkaP369S77169fr379+vkpKgCAL4X4OwA0DZMnT9bdd9+tXr16qW/fvnrppZd06NAhPfDAA/4ODfDa8ePH9c033zhf5+fna+fOnYqJiVH79u39GBlw4bDsEA22cOFCzZkzR0ePHlVSUpLmzZunG2+80d9hAV774IMPdNNNN9XbP27cOGVnZ1/4gAA/ICEAAADMIQAAACQEAABAJAQAAEAkBAAAQCQEAABAJAQAAEAkBAAAQCQEAABAJASA12bOnKmrrrrK+fqee+7RyJEjL3gcBw4ckMVi0c6dO886pmPHjsrKymrwNbOzs9W8eXOvY7NYLFqzZo3X1wHQeEgIEJDuueceWSwWWSwWhYaGqlOnTpo6darKy8sb/Wc///zzDb7dbUM+xAHgQuDhRghYN998s5YtW6aamhp99NFHuvfee1VeXq5FixbVG1tTU6PQ0FCf/FybzeaT6wDAhUSFAAHLarXKbrcrISFBY8eO1Z133uksW58q8//5z39Wp06dZLVaZRiGSkpKdP/99ys2NlbR0dH6r//6L33xxRcu13322WcVFxenqKgopaamqrKy0uX46S0Dh8Oh2bNn67LLLpPValX79u01a9YsSVJiYqIkqWfPnrJYLBowYIDzvGXLlqlr164KDw9Xly5dtHDhQpef89lnn6lnz54KDw9Xr169tGPHDo9/R3PnzlVycrKaNWumhIQETZgwQcePH683bs2aNbr88ssVHh6uwYMHq6CgwOX4W2+9pZSUFIWHh6tTp0566qmnVFtb63E8APyHhACmERERoZqaGufrb775RqtXr9brr7/uLNnfeuutKiws1Ntvv628vDxdffXVGjhwoH766SdJ0urVq/Xkk09q1qxZ2r59u9q0aVPvg/p0M2bM0OzZs/X4449r7969WrlypeLi4iSd/FCXpA0bNujo0aP661//KklasmSJ0tPTNWvWLO3bt08ZGRl6/PHHlZOTI0kqLy/X8OHDdcUVVygvL08zZ87U1KlTPf6dBAUF6YUXXtDu3buVk5Oj9957T9OmTXMZc+LECc2aNUs5OTn6+OOPVVpaqjFjxjiPv/vuu7rrrrs0adIk7d27V4sXL1Z2drYz6QHQRBhAABo3bpwxYsQI5+tPP/3UaNmypTF69GjDMAzjySefNEJDQ42ioiLnmI0bNxrR0dFGZWWly7UuvfRSY/HixYZhGEbfvn2NBx54wOV4nz59jB49epzxZ5eWlhpWq9VYsmTJGePMz883JBk7duxw2Z+QkGCsXLnSZd8zzzxj9O3b1zAMw1i8eLERExNjlJeXO48vWrTojNf6Tx06dDDmzZt31uOrV682WrZs6Xy9bNkyQ5KxdetW5759+/YZkoxPP/3UMAzDuOGGG4yMjAyX6yxfvtxo06aN87UkIzc396w/F4D/MYcAAetvf/ubLrnkEtXW1qqmpkYjRozQ/Pnzncc7dOig1q1bO1/n5eXp+PHjatmypct1Kioq9O2330qS9u3bpwceeMDleN++ffX++++fMYZ9+/apqqpKAwcObHDcx44dU0FBgVJTU3Xfffc599fW1jrnJ+zbt089evRQZGSkSxyeev/995WRkaG9e/eqtLRUtbW1qqysVHl5uZo1ayZJCgkJUa9evZzndOnSRc2bN9e+fft0zTXXKC8vT9u2bXOpCNTV1amyslInTpxwiRHAxYuEAAHrpptu0qJFixQaGqr4+Ph6kwZPfeCd4nA41KZNG33wwQf1rnW+S+8iIiI8PsfhcEg62Tbo06ePy7Hg4GBJkmEY5xXPfzp48KBuueUWPfDAA3rmmWcUExOjzZs3KzU11aW1Ip1cNni6U/scDoeeeuopjRo1qt6Y8PBwr+MEcGGQECBgNWvWTJdddlmDx1999dUqLCxUSEiIOnbseMYxXbt21datW/W///u/zn1bt2496zU7d+6siIgIbdy4Uffee2+942FhYZJOfqM+JS4uTm3bttV3332nO++884zX7datm5YvX66Kigpn0uEujjPZvn27amtr9dxzzyko6OR0otWrV9cbV1tbq+3bt+uaa66RJO3fv18///yzunTpIunk723//v0e/a4BXHxICIB/GTRokPr27auRI0dq9uzZuuKKK3TkyBG9/fbbGjlypHr16qVHHnlE48aNU69evXT99dfr5Zdf1p49e9SpU6czXjM8PFzTp0/XtGnTFBYWpuuuu07Hjh3Tnj17lJqaqtjYWEVERGjt2rVq166dwsPDZbPZNHPmTE2aNEnR0dEaNmyYqqqqtH37dhUXF2vy5MkaO3as0tPTlZqaqscee0wHDhzQH/7wB4/e76WXXqra2lrNnz9ft912mz7++GO9+OKL9caFhobq4Ycf1gsvvKDQ0FA99NBDuvbaa50JwhNPPKHhw4crISFBv/zlLxUUFKQvv/xSu3bt0u9+9zvP/4cA4BesMgD+xWKx6O2339aNN96oX//617r88ss1ZswYHThwwLkq4I477tATTzyh6dOnKyUlRQcPHtSDDz7o9rqPP/64pkyZoieeeEJdu3bVHXfcoaKiIkkn+/MvvPCCFi9erPj4eI0YMUKSdO+99+pPf/qTsrOzlZycrP79+ys7O9u5TPGSSy7RW2+9pb1796pnz55KT0/X7NmzPXq/V111lebOnavZs2crKSlJL7/8sjIzM+uNi4yM1PTp0zV27Fj17dtXERERWrVqlfP40KFD9be//U3r169X7969de2112ru3Lnq0KGDR/EA8C+L4YtmJAAAaNKoEAAAABICAABAQgAAAERCAAAAREIAAABEQgAAAERCAAAAREIAAABEQgAAAERCAAAAREIAAAAk/X8yrMcXPA2VaAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b28d75a-6692-45b6-ab5c-30a13ff54f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"best_transformer_wcgan_ev.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8823084c-57ab-4edd-b656-345792a20ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN_LSTM_IDS(nn.Module):\n",
    "    def __init__(self, input_dim, conv_channels=32, lstm_hidden=64, lstm_layers=1, dropout=0.2):\n",
    "        super(CNN_LSTM_IDS, self).__init__()\n",
    "        # Expect input: (batch, seq_len, features). If seq_len==1, treat features as 1D series.\n",
    "        # We'll interpret features as a 1D \"signal\" for Conv1d by reshaping.\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=conv_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(conv_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveMaxPool1d(output_size=8)  # reduce to fixed length\n",
    "        \n",
    "        # LSTM works over time steps; we'll treat conv output as seq_len for LSTM\n",
    "        self.lstm = nn.LSTM(input_size=conv_channels, hidden_size=lstm_hidden, num_layers=lstm_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden*2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # Accept x shape: (batch, features) or (batch, seq_len, features)\n",
    "        if x.dim() == 2:\n",
    "            # (batch, features) -> (batch, 1, features)\n",
    "            x = x.unsqueeze(1)\n",
    "        # if x is (batch, seq_len, features) we may collapse seq_len with features or handle differently.\n",
    "        # Here we flatten seq_len dimension into features axis if needed:\n",
    "        if x.dim() == 3 and x.size(1) > 1:\n",
    "            # flatten seq_len into feature axis: (batch, 1, seq_len*features)\n",
    "            b, s, f = x.size()\n",
    "            x = x.reshape(b, 1, s*f)\n",
    "        # Now shape (batch, 1, L)\n",
    "        x = self.conv1(x)           # (batch, conv_channels, L)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)            # (batch, conv_channels, 8)\n",
    "        # prepare for LSTM: we want (batch, seq_len, features) -> treat 8 as seq_len\n",
    "        x = x.permute(0, 2, 1)      # (batch, 8, conv_channels)\n",
    "        out, _ = self.lstm(x)       # (batch, 8, 2*lstm_hidden)\n",
    "        out = out.mean(dim=1)       # (batch, 2*lstm_hidden)\n",
    "        out = self.fc(out)          # (batch, 1)\n",
    "        return out\n",
    "model = CNN_LSTM_IDS(input_dim=X_train_tensor.shape[1]).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# use DataLoader on X_train_tensor, y_train_tensor (2D tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f6e0add-7dd7-45c3-9175-1fe623dffee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimension: 10\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "print(\"Input dimension:\", input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04f6cbaa-2643-4b1e-9567-5b13b7ca9503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.24.0-cp311-cp311-win_amd64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Collecting torch==2.9.0 (from torchvision)\n",
      "  Downloading torch-2.9.0-cp311-cp311-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.9.0->torchvision) (3.13.1)\n",
      "Collecting typing-extensions>=4.10.0 (from torch==2.9.0->torchvision)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting sympy>=1.13.3 (from torch==2.9.0->torchvision)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.9.0->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.9.0->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.9.0->torchvision) (2023.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch==2.9.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch==2.9.0->torchvision) (2.1.3)\n",
      "Downloading torchvision-0.24.0-cp311-cp311-win_amd64.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.9/4.0 MB 41.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.0/4.0 MB 42.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.0/4.0 MB 32.1 MB/s eta 0:00:00\n",
      "Downloading torch-2.9.0-cp311-cp311-win_amd64.whl (109.3 MB)\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.3/109.3 MB 48.1 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 4.3/109.3 MB 55.4 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 6.8/109.3 MB 54.1 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 7.5/109.3 MB 47.6 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 7.6/109.3 MB 34.6 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 8.1/109.3 MB 28.7 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 9.9/109.3 MB 30.2 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 11.1/109.3 MB 28.5 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 11.6/109.3 MB 26.2 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 23.4 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 12.5/109.3 MB 21.1 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 12.9/109.3 MB 20.5 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 19.3 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 13.7/109.3 MB 18.2 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 13.8/109.3 MB 16.0 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 14.1/109.3 MB 15.6 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 14.3/109.3 MB 14.5 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 14.5/109.3 MB 13.6 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 14.5/109.3 MB 13.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 14.5/109.3 MB 13.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 14.5/109.3 MB 13.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 14.5/109.3 MB 13.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 14.5/109.3 MB 13.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 14.5/109.3 MB 13.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 14.5/109.3 MB 13.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 14.5/109.3 MB 13.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 14.5/109.3 MB 13.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 14.8/109.3 MB 8.6 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 15.2/109.3 MB 8.3 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 16.2/109.3 MB 8.1 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 17.4/109.3 MB 8.0 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 18.8/109.3 MB 8.8 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 20.1/109.3 MB 8.7 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 22.3/109.3 MB 9.5 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 23.2/109.3 MB 10.1 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 23.6/109.3 MB 9.9 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 24.0/109.3 MB 10.6 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 24.4/109.3 MB 11.1 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 24.8/109.3 MB 21.1 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 25.3/109.3 MB 21.1 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 25.6/109.3 MB 19.8 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 26.0/109.3 MB 19.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 26.5/109.3 MB 18.7 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 27.2/109.3 MB 17.7 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 27.7/109.3 MB 16.8 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 28.1/109.3 MB 16.0 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 15.2 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 29.2/109.3 MB 14.2 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 13.4 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 30.0/109.3 MB 13.1 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 30.5/109.3 MB 12.6 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 30.9/109.3 MB 12.1 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 31.3/109.3 MB 11.5 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 11.1 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 32.2/109.3 MB 10.7 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 32.7/109.3 MB 10.2 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 33.2/109.3 MB 10.1 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 33.6/109.3 MB 9.9 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 34.1/109.3 MB 9.9 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 34.7/109.3 MB 9.9 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 35.2/109.3 MB 9.9 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 35.7/109.3 MB 10.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 36.1/109.3 MB 10.2 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 36.6/109.3 MB 10.2 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 37.1/109.3 MB 10.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 37.4/109.3 MB 9.8 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 37.9/109.3 MB 9.8 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 38.4/109.3 MB 9.9 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 38.9/109.3 MB 9.9 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 39.5/109.3 MB 9.9 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 39.9/109.3 MB 9.9 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 40.3/109.3 MB 9.9 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 40.8/109.3 MB 9.9 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 41.3/109.3 MB 9.9 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 41.8/109.3 MB 10.1 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 42.3/109.3 MB 10.1 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 42.7/109.3 MB 9.9 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 43.2/109.3 MB 10.1 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 43.6/109.3 MB 10.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 44.2/109.3 MB 10.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 44.6/109.3 MB 9.9 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 45.0/109.3 MB 9.8 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 45.5/109.3 MB 10.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 46.0/109.3 MB 10.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 46.4/109.3 MB 9.9 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 47.1/109.3 MB 9.9 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 47.5/109.3 MB 10.1 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 48.0/109.3 MB 10.1 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 48.3/109.3 MB 9.9 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 48.8/109.3 MB 9.9 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 49.3/109.3 MB 10.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 49.8/109.3 MB 9.9 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 50.2/109.3 MB 10.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 50.7/109.3 MB 10.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 51.3/109.3 MB 9.8 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 51.9/109.3 MB 10.1 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 52.3/109.3 MB 9.9 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 52.6/109.3 MB 9.9 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 52.8/109.3 MB 9.6 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 53.2/109.3 MB 9.6 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 53.6/109.3 MB 9.6 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 53.7/109.3 MB 9.5 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 54.0/109.3 MB 9.1 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 54.3/109.3 MB 9.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 54.8/109.3 MB 9.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 55.4/109.3 MB 9.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 56.2/109.3 MB 9.5 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 57.0/109.3 MB 10.1 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 57.6/109.3 MB 9.9 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 57.9/109.3 MB 10.1 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 58.4/109.3 MB 9.9 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 59.0/109.3 MB 10.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 59.5/109.3 MB 9.9 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 59.8/109.3 MB 9.9 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 60.3/109.3 MB 10.1 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 60.8/109.3 MB 9.9 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 61.3/109.3 MB 9.9 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 61.7/109.3 MB 9.9 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 62.3/109.3 MB 9.9 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 62.7/109.3 MB 9.9 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 63.1/109.3 MB 10.4 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 63.6/109.3 MB 10.4 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 64.0/109.3 MB 10.7 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 64.6/109.3 MB 11.1 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 65.1/109.3 MB 10.9 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 65.4/109.3 MB 10.7 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 65.9/109.3 MB 10.7 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 66.4/109.3 MB 10.4 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 66.8/109.3 MB 10.1 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 67.3/109.3 MB 9.9 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 67.8/109.3 MB 10.1 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 68.2/109.3 MB 9.9 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 68.7/109.3 MB 10.1 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 69.3/109.3 MB 9.9 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 69.8/109.3 MB 9.9 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 70.3/109.3 MB 9.9 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 70.7/109.3 MB 9.8 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 71.3/109.3 MB 9.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 71.7/109.3 MB 9.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 72.1/109.3 MB 9.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 72.6/109.3 MB 10.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 73.0/109.3 MB 9.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 73.6/109.3 MB 9.9 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 74.0/109.3 MB 9.9 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 74.5/109.3 MB 9.9 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 75.0/109.3 MB 9.9 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 75.5/109.3 MB 10.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 75.9/109.3 MB 10.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 76.3/109.3 MB 9.9 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 76.8/109.3 MB 9.9 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 77.3/109.3 MB 10.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 77.7/109.3 MB 9.9 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 78.2/109.3 MB 10.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 78.6/109.3 MB 9.9 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 79.1/109.3 MB 9.9 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 79.5/109.3 MB 10.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 80.0/109.3 MB 10.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 80.5/109.3 MB 10.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 80.9/109.3 MB 10.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 81.3/109.3 MB 9.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 81.9/109.3 MB 9.9 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 82.3/109.3 MB 9.9 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 82.9/109.3 MB 10.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 83.3/109.3 MB 10.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 83.9/109.3 MB 10.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 84.2/109.3 MB 9.8 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 84.6/109.3 MB 9.9 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 85.0/109.3 MB 9.9 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 85.6/109.3 MB 9.9 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 86.0/109.3 MB 10.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 86.4/109.3 MB 10.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 86.9/109.3 MB 9.9 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 87.5/109.3 MB 10.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 88.0/109.3 MB 9.9 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 88.3/109.3 MB 9.9 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 88.7/109.3 MB 9.6 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 89.2/109.3 MB 9.8 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 89.6/109.3 MB 9.6 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 89.9/109.3 MB 9.5 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 90.5/109.3 MB 9.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 91.3/109.3 MB 9.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 91.8/109.3 MB 9.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 92.3/109.3 MB 10.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 92.7/109.3 MB 9.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 93.3/109.3 MB 10.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 93.9/109.3 MB 10.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 94.2/109.3 MB 9.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 94.5/109.3 MB 9.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 94.9/109.3 MB 9.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 95.5/109.3 MB 9.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 96.2/109.3 MB 10.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 96.7/109.3 MB 10.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 97.1/109.3 MB 9.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 97.6/109.3 MB 9.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 98.1/109.3 MB 9.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 98.4/109.3 MB 9.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 98.9/109.3 MB 10.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 99.4/109.3 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 99.9/109.3 MB 10.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 100.4/109.3 MB 10.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 100.8/109.3 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 101.3/109.3 MB 9.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 101.9/109.3 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 102.4/109.3 MB 9.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 102.8/109.3 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 103.3/109.3 MB 10.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 103.9/109.3 MB 9.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 104.3/109.3 MB 9.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 104.3/109.3 MB 9.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 104.4/109.3 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 104.9/109.3 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 105.5/109.3 MB 9.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 106.1/109.3 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  106.8/109.3 MB 9.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  107.6/109.3 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  108.0/109.3 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  108.5/109.3 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  109.0/109.3 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  109.3/109.3 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  109.3/109.3 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  109.3/109.3 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  109.3/109.3 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  109.3/109.3 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  109.3/109.3 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  109.3/109.3 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  109.3/109.3 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  109.3/109.3 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  109.3/109.3 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 109.3/109.3 MB 6.9 MB/s eta 0:00:00\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: typing-extensions, sympy, torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.1\n",
      "    Uninstalling torch-2.3.1:\n",
      "      Successfully uninstalled torch-2.3.1\n",
      "Successfully installed sympy-1.14.0 torch-2.9.0 torchvision-0.24.0 typing-extensions-4.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script isympy.exe is installed in 'C:\\Users\\SANDIP\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\SANDIP\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\SANDIP\\AppData\\Roaming\\Python\\Python311\\site-packages\\~orch'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9a1b7e0-69b2-4a8f-8425-06517eef8ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp311-cp311-win_amd64.whl (2449.4 MB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp311-cp311-win_amd64.whl (6.1 MB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp311-cp311-win_amd64.whl (4.1 MB)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sandip\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sandip\\appdata\\roaming\\python\\python311\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe, torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\SANDIP\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "868c3e6d-8c96-4c97-854c-a7a3acb0f9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.5.1+cu121\n",
      "TorchVision: 0.20.1+cu121\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"TorchVision:\", torchvision.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7eedfff2-65ad-484d-b8d8-e0e213e22249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "def window_to_image(window):\n",
    "    \"\"\"\n",
    "    window: 1D numpy array of length L (features)\n",
    "    return: torch tensor shape (C,H,W) normalized [0,1]\n",
    "    \"\"\"\n",
    "    # reshape to near-square matrix: choose H = int(sqrt(L)) or a fixed H\n",
    "    L = window.shape[0]\n",
    "    H = int(np.ceil(np.sqrt(L)))\n",
    "    # pad\n",
    "    pad = H*H - L\n",
    "    arr = np.pad(window, (0, pad), 'constant', constant_values=(0,))\n",
    "    mat = arr.reshape(H, H)\n",
    "    # normalize mat to 0-255 for image\n",
    "    mat = (mat - mat.min()) / (mat.max()-mat.min() + 1e-8)\n",
    "    mat = (mat * 255).astype(np.uint8)\n",
    "    img = Image.fromarray(mat)\n",
    "    # convert to 3-channel RGB\n",
    "    img = img.convert(\"RGB\").resize((64,64))\n",
    "    tf = transforms.Compose([\n",
    "        transforms.ToTensor()  # -> [0,1] float tensor (C,H,W)\n",
    "    ])\n",
    "    return tf(img)\n",
    "\n",
    "# Example: create dataset of images (torch tensors) from X_train numpy\n",
    "X_train_np = X_train_tensor.cpu().numpy()\n",
    "img_tensors = [window_to_image(x) for x in X_train_np]\n",
    "X_img = torch.stack(img_tensors)  # (N, C, H, W)\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3,32,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*16*16, 128), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(128,1), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2fc9b8c-d3f7-4c33-93a9-2d0d927266df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ensemble(models, scalers, raw_vector):\n",
    "    \"\"\"\n",
    "    models: dict {'transformer': tmodel, 'cnn_lstm': m1, 'img_cnn': m2}\n",
    "    scalers: scaler used for normalization\n",
    "    raw_vector: 1d array scaled raw\n",
    "    \"\"\"\n",
    "    # prepare inputs\n",
    "    x = np.array(raw_vector).reshape(1, -1)\n",
    "    x_scaled = scalers.transform(x)  # keep scaler used for training\n",
    "    x_tensor = torch.tensor(x_scaled, dtype=torch.float32).to(device)\n",
    "    # transformer expects (batch,features) -> works\n",
    "    p1 = models['transformer'](x_tensor).detach().cpu().numpy().item()\n",
    "    p2 = models['cnn_lstm'](x_tensor).detach().cpu().numpy().item()\n",
    "    # image\n",
    "    img_t = window_to_image(x_scaled.flatten()).unsqueeze(0).to(device)\n",
    "    p3 = models['img_cnn'](img_t).detach().cpu().numpy().item()\n",
    "    # average probabilities\n",
    "    p_mean = (p1 + p2 + p3) / 3.0\n",
    "    label = \"attack\" if p_mean >= 0.5 else \"normal\"\n",
    "    return {\"prob\": p_mean, \"label\": label, \"components\": [p1,p2,p3]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29f413f3-19cd-4cd1-87b9-e1d7fcc4c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = TransformerBinaryClassifier(d_model=10)\n",
    "cnn_lstm_model = CNN_LSTM_IDS(input_dim=10)\n",
    "img_cnn_model = SimpleCNN()\n",
    "\n",
    "torch.save(transformer_model.state_dict(), r\"F:\\INTRUSION DETECTION DATASET\\transformer_model.pth\")\n",
    "torch.save(cnn_lstm_model.state_dict(), r\"F:\\INTRUSION DETECTION DATASET\\cnn_lstm_model.pth\")\n",
    "torch.save(img_cnn_model.state_dict(), r\"F:\\INTRUSION DETECTION DATASET\\img_cnn_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3701fa5b-b0fc-49d5-bf28-804dbe96038c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Example during preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # or fit_transform\n",
    "\n",
    "# Save it\n",
    "joblib.dump(scaler, r\"F:\\INTRUSION DETECTION DATASET\\scaler.pkl\")\n",
    "print(\"Scaler saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b2339a-b41e-4f9e-9742-5de65e49e446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
